[
    {
        "instruction": "You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks.\nGiven a user request, you are expected to write a complete program that accomplishes the requested task and save any outputs in the correct format.\nPlease wrap your program in a code block that specifies the script type, python. For example:\n```python\nprint(\"Hello World!\")\n```\n",
        "input": "Analyze the performance of different algorithms on the Hurricanes Dataset by loading data from multiple CSV files located in the \"benchmark/datasets/HurricanesDataset\" directory. Specifically, you will evaluate the \"Harmonic mean\" metric across various window sizes (5, 7, 9, 11, and 13) for three algorithms: Cubic, Random Walk, and Linear Regression. Save the combined dataset to \"pred_results/compare_es.py_pred.csv\". Generate a boxplot comparing the harmonic mean values for each algorithm and window size, and save this visualization as \"pred_results/compare_es.py_pred.png\". Additionally, compute and save correlation statistics, including median and standard deviation for each algorithm and window size, to \"pred_results/compare_es.py_pred_correlations.csv\". Finally, summarize the analysis in a text file named \"pred_results/compare_es.py_pred.txt\", detailing the total data points, algorithms used, window sizes, and a summary for each algorithm's performance.",
        "output": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom scipy.spatial import distance\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create output directory if it doesn't exist\noutput_dir = \"pred_results/\"\nos.makedirs(output_dir, exist_ok=True)\n\ndef load_df(col='Harmonic mean'):\n    df = pd.DataFrame()\n    \n    # Define the base directory for input files\n    base_dir = \"benchmark/datasets/HurricanesDataset\"\n    \n    # List of file patterns and their corresponding algorithm and window size\n    file_configs = [\n        # Cubic algorithm\n        ('Results_HurricanesDataset_SWS_C_WS_5.csv', 'Cubic', 5),\n        ('Results_HurricanesDataset_SWS_C_WS_7.csv', 'Cubic', 7),\n        ('Results_HurricanesDataset_SWS_C_WS_9.csv', 'Cubic', 9),\n        ('Results_HurricanesDataset_SWS_C_WS_11.csv', 'Cubic', 11),\n        ('Results_HurricanesDataset_SWS_C_WS_13.csv', 'Cubic', 13),\n        \n        # Random Walk algorithm\n        ('Results_HurricanesDataset_SWS_RW_WS_5.csv', 'Random Walk', 5),\n        ('Results_HurricanesDataset_SWS_RW_WS_7.csv', 'Random Walk', 7),\n        ('Results_HurricanesDataset_SWS_RW_WS_9.csv', 'Random Walk', 9),\n        ('Results_HurricanesDataset_SWS_RW_WS_11.csv', 'Random Walk', 11),\n        ('Results_HurricanesDataset_SWS_RW_WS_13.csv', 'Random Walk', 13),\n        \n        # Linear Regression algorithm\n        ('Results_HurricanesDataset_SWS_LR_WS_5.csv', 'Linear Regression', 5),\n        ('Results_HurricanesDataset_SWS_LR_WS_7.csv', 'Linear Regression', 7),\n        ('Results_HurricanesDataset_SWS_LR_WS_9.csv', 'Linear Regression', 9),\n        ('Results_HurricanesDataset_SWS_LR_WS_11.csv', 'Linear Regression', 11),\n        ('Results_HurricanesDataset_SWS_LR_WS_13.csv', 'Linear Regression', 13),\n    ]\n    \n    # Load each file and append to the dataframe\n    for file_name, algorithm, ws in file_configs:\n        file_path = os.path.join(base_dir, file_name)\n        try:\n            df2 = pd.read_csv(file_path)\n            df2 = pd.DataFrame(df2[col].copy())\n            df2 = df2.assign(ws=[ws] * df2.shape[0])\n            df2 = df2.assign(Algorithms=[algorithm] * df2.shape[0])\n            if df.empty:\n                df = df2\n            else:\n                df = pd.concat([df, df2], ignore_index=True)\n        except FileNotFoundError:\n            print(f\"Warning: File {file_path} not found. Skipping.\")\n    \n    return df, []\n\ndef plot(df, label, xcl):\n    # Set up the plot style\n    SMALL_SIZE = 24\n    MEDIUM_SIZE = 26\n    BIGGER_SIZE = 28\n\n    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n    \n    sns.set_style(\"white\")\n    fig = plt.figure(figsize=(16, 12))\n    \n    # Create the boxplot\n    ax = sns.boxplot(x=\"ws\", y='Harmonic mean',\n                hue=\"Algorithms\", data=df, orient=\"v\", notch=False)\n    \n    plt.ylabel(label)\n    plt.xlabel(\"Window Size\")\n    plt.tight_layout()\n    plt.title(\"Hurricanes Dataset\")\n    \n    # Save the plot\n    plot_path = os.path.join(output_dir, \"compare_es.py_pred.png\")\n    plt.savefig(plot_path)\n    \n    # Close the figure to free memory\n    plt.close(fig)\n\ndef analyze_correlations(df):\n    correlations = []\n    \n    # For each algorithm, analyze the correlation between window size and harmonic mean\n    for algorithm, group in df.groupby(['Algorithms']):\n        for ws, subgroup in group.groupby('ws'):\n            # Calculate median and std of the Harmonic mean for this algorithm and window size\n            h_mean_values = subgroup['Harmonic mean'].values\n            median_val = np.median(h_mean_values)\n            std_val = np.std(h_mean_values)\n            \n            correlations.append({\n                'Algorithm': algorithm,\n                'Window Size': ws,\n                'Median': median_val,\n                'Std': std_val\n            })\n            print(f\"{algorithm}, {ws}, {median_val}, {std_val}\")\n        \n        # Calculate correlations\n        x = group['Harmonic mean'].values\n        y = group['ws'].values\n        \n        try:\n            import dcor\n            dcor_stats = dcor.distance_stats(x, y)\n            print(f\"dcor: {dcor_stats}\")\n        except ImportError:\n            print(\"dcor module not available\")\n        \n        try:\n            dist_corr = distance.correlation(x, y)\n            print(f\"distance correlation: {dist_corr}\")\n        except:\n            print(\"Error calculating distance correlation\")\n        \n        try:\n            from minepy import MINE\n            mine = MINE(alpha=0.6, c=15, est=\"mic_approx\")\n            mine.compute_score(x, y)\n            mic = mine.mic()\n            print(f\"Maximal Information Coefficient: {mic}\")\n        except ImportError:\n            print(\"minepy module not available\")\n        \n        pearson_corr = np.corrcoef(group['Harmonic mean'].values, group['ws'].values)[1][0]\n        print(f\"{algorithm} Pearson correlation: {pearson_corr}\")\n    \n    # Save correlations to CSV\n    correlations_df = pd.DataFrame(correlations)\n    correlations_csv_path = os.path.join(output_dir, \"compare_es.py_pred_correlations.csv\")\n    correlations_df.to_csv(correlations_csv_path, index=False)\n    \n    return correlations_df\n\ndef main():\n    # Load data\n    dfh, xcl = load_df('Harmonic mean')\n    \n    # Save the loaded data to CSV\n    csv_path = os.path.join(output_dir, \"compare_es.py_pred.csv\")\n    dfh.to_csv(csv_path, index=False)\n    \n    # Generate plot\n    plot(dfh, 'Harmonic mean', xcl)\n    \n    # Analyze correlations\n    correlations_df = analyze_correlations(dfh)\n    \n    # Write output summary\n    with open(os.path.join(output_dir, \"compare_es.py_pred.txt\"), \"w\") as f:\n        f.write(\"Analysis of Hurricanes Dataset with different algorithms and window sizes\\n\")\n        f.write(\"=\" * 80 + \"\\n\\n\")\n        f.write(f\"Total data points: {len(dfh)}\\n\")\n        f.write(f\"Algorithms: {', '.join(dfh['Algorithms'].unique())}\\n\")\n        f.write(f\"Window sizes: {', '.join(map(str, sorted(dfh['ws'].unique())))}\\n\\n\")\n        \n        for algorithm in dfh['Algorithms'].unique():\n            f.write(f\"\\nSummary for {algorithm}:\\n\")\n            f.write(\"-\" * 40 + \"\\n\")\n            algo_data = correlations_df[correlations_df['Algorithm'] == algorithm]\n            for _, row in algo_data.iterrows():\n                f.write(f\"Window Size {row['Window Size']}: Median = {row['Median']:.4f}, Std = {row['Std']:.4f}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n```",
        "discipline": "geography",
        "file_url": "https://github.com/metemaad/TrajSeg/blob/master/Experiments/HurricanesDataset/compare_es.py",
        "output_fname": "generated_gold_programs_yifei_partition/TrajSeg_compare_es.py"
    },
    {
        "instruction": "You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks.\nGiven a user request, you are expected to write a complete program that accomplishes the requested task and save any outputs in the correct format.\nPlease wrap your program in a code block that specifies the script type, python. For example:\n```python\nprint(\"Hello World!\")\n```\n",
        "input": "Analyze the mean absolute error (MAE) and root mean square error (RMSE) for molecular predictions using the data from the collection file located at \"benchmark/datasets/physical_properties/logP/analysis_of_extra_molecules/analysis_outputs_withrefs/logP_submission_collection.csv\". The analysis will be performed for all prediction methods as well as for specific categories: \"Physical\" and \"Empirical\". Save the results, including statistical tables and plots, in the directory \"pred_results/logP_analysis2_outputs/MolecularStatisticsTables\". The output files should include \"molecular_error_statistics.csv\" for overall statistics, \"molecular_error_statistics_for_Physical_methods.csv\" and \"molecular_error_statistics_for_Empirical_methods.csv\" for category-specific statistics, a comparison plot named \"molecular_MAE_comparison_between_method_categories.pdf\", and a ridge plot for error distributions named \"molecular_error_distribution_ridge_plot.pdf\".",
        "output": "```python\n#!/usr/bin/env python\n\n# =============================================================================\n# GLOBAL IMPORTS\n# =============================================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\nimport joypy\nimport scipy.stats\n\n# =============================================================================\n# UTILITY FUNCTIONS FROM logP_analysis.py\n# =============================================================================\n\ndef mae(data):\n    \"\"\"Calculate mean absolute error.\n\n    Parameters\n    ----------\n    data : ndarray\n        An array whose first dimension is over samples. Each sample is\n        expected to be a tuple with an experimental and a calculated value.\n\n    Returns\n    -------\n    float\n        Mean absolute error over the samples.\n\n    \"\"\"\n    signed_errors = np.array([datum[0] - datum[1] for datum in data])\n    return np.mean(np.abs(signed_errors))\n\ndef rmse(data):\n    \"\"\"Calculate root mean square error.\n\n    Parameters\n    ----------\n    data : ndarray\n        An array whose first dimension is over samples. Each sample is\n        expected to be a tuple with an experimental and a calculated value.\n\n    Returns\n    -------\n    float\n        Root mean square error over the samples.\n\n    \"\"\"\n    signed_errors = np.array([datum[0] - datum[1] for datum in data])\n    return np.sqrt(np.mean(np.square(signed_errors)))\n\ndef compute_bootstrap_statistics(samples, stats_funcs, percentile=0.95,\n                                n_bootstrap_samples=1000):\n    \"\"\"Compute bootstrap confidence interval for the given statistics functions.\n\n    Parameters\n    ----------\n    samples : np.ndarray\n        The array of samples.\n    stats_funcs : List[function]\n        The statistics functions for which to compute the confidence interval.\n    percentile : float, optional\n        The percentile of the confidence interval (default is 0.95).\n    n_bootstrap_samples : int, optional\n        Number of bootstrap samples to generate (default is 1000).\n\n    Returns\n    -------\n    confidence_intervals : List[Tuple[float, Tuple[float, float]]]\n        stats_func(samples) and the confidence interval for each statistics\n        function. Each element is a tuple with the statistics computed on the\n        full sample and the confidence interval (lower_bound, upper_bound).\n\n    \"\"\"\n    # Handle case where only a single sample is passed.\n    try:\n        n_samples = len(samples)\n    except TypeError:\n        samples = [samples]\n        n_samples = len(samples)\n\n    # Compute mean statistics.\n    statistics = [stats_func(samples) for stats_func in stats_funcs]\n\n    # Generate bootstrap statistics.\n    bootstrap_samples_statistics = np.zeros((len(stats_funcs), n_bootstrap_samples))\n    for bootstrap_sample_idx in range(n_bootstrap_samples):\n        # Re-sample with replacement.\n        bootstrap_samples = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n        bootstrap_samples = [samples[i] for i in bootstrap_samples]\n        # Compute statistics.\n        for stats_func_idx, stats_func in enumerate(stats_funcs):\n            bootstrap_samples_statistics[stats_func_idx][bootstrap_sample_idx] = stats_func(bootstrap_samples)\n\n    # Compute confidence intervals.\n    percentile_index = int(np.round(n_bootstrap_samples * (1 - percentile) / 2)) - 1\n    if percentile_index < 0:\n        percentile_index = 0\n    confidence_intervals = []\n    for stats_func_idx, samples_statistics in enumerate(bootstrap_samples_statistics):\n        samples_statistics.sort()\n        stat = statistics[stats_func_idx]\n        ci = (samples_statistics[percentile_index],\n              samples_statistics[-percentile_index-1])\n        confidence_intervals.append((stat, ci))\n    return confidence_intervals\n\ndef barplot_with_CI_errorbars(df, x_label, y_label, y_lower_label, y_upper_label, figsize=None):\n    \"\"\"Creates bar plot of a given dataframe with asymmetric error bars for y axis.\n\n    Args:\n        df: Pandas Dataframe that should have columns with columnnames specified in other arguments.\n        x_label: str, column name of x axis categories\n        y_label: str, column name of y axis values\n        y_lower_label: str, column name of lower error values of y axis\n        y_upper_label: str, column name of upper error values of y axis\n    \"\"\"\n    # Column names for new columns for delta y_err which is calculated as | y_err - y |\n    delta_lower_yerr_label = \"$\\Delta$\" + y_lower_label\n    delta_upper_yerr_label = \"$\\Delta$\" + y_upper_label\n    data = df  # Pandas DataFrame\n    data[delta_lower_yerr_label] = data[y_label] - data[y_lower_label]\n    data[delta_upper_yerr_label] = data[y_upper_label] - data[y_label]\n\n    # Plot style\n    plt.close()\n    # Updated style settings to be compatible with newer matplotlib versions\n    plt.style.use(['default'])\n    sns.set_style(\"whitegrid\")\n    plt.rcParams['axes.labelsize'] = 18\n    plt.rcParams['xtick.labelsize'] = 14\n    plt.rcParams['ytick.labelsize'] = 16\n    \n    # Create figure\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(8, 6))\n\n    # Plot\n    x = range(len(data[y_label]))\n    y = data[y_label]\n    plt.bar(x, y)\n    plt.xticks(x, data[x_label], rotation=90)\n    plt.errorbar(x, y, yerr=(data[delta_lower_yerr_label], data[delta_upper_yerr_label]),\n                 fmt=\"none\", ecolor=\"black\", capsize=3, capthick=True)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.tight_layout()\n\n# =============================================================================\n# PLOTTING FUNCTIONS\n# =============================================================================\n\ndef barplot_with_CI_errorbars_and_2groups(df1, df2, x_label, y_label, y_lower_label, y_upper_label):\n    \"\"\"Creates bar plot of a given dataframe with asymmetric error bars for y axis.\n    Args:\n        df: Pandas Dataframe that should have columns with columnnames specified in other arguments.\n        x_label: str, column name of x axis categories\n        y_label: str, column name of y axis values\n        y_lower_label: str, column name of lower error values of y axis\n        y_upper_label: str, column name of upper error values of y axis\n    \"\"\"\n    # Column names for new columns for delta y_err which is calculated as | y_err - y |\n    delta_lower_yerr_label = \"$\\Delta$\" + y_lower_label\n    delta_upper_yerr_label = \"$\\Delta$\" + y_upper_label\n\n    # Plot style\n    plt.close()\n    # Updated style settings to be compatible with newer matplotlib versions\n    plt.style.use(['default'])\n    sns.set_style(\"whitegrid\")\n    plt.rcParams['axes.labelsize'] = 18\n    plt.rcParams['xtick.labelsize'] = 12\n    plt.rcParams['ytick.labelsize'] = 16\n    plt.tight_layout()\n    bar_width = 0.4\n\n    # Zesty colorblind-friendly color palette\n    color0 = \"#0F2080\"\n    color1 = \"#F5793A\"\n    color2 = \"#A95AA1\"\n    color3 = \"#85C0F9\"\n    current_palette = [color0, color1, color2, color3]\n    error_color = 'gray'\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    # Plot 1st group of data\n    data = df1  # Pandas DataFrame\n    data[delta_lower_yerr_label] = data[y_label] - data[y_lower_label]\n    data[delta_upper_yerr_label] = data[y_upper_label] - data[y_label]\n\n    x = range(len(data[y_label]))\n    y = data[y_label]\n    ax.bar(x, y, label = \"QM\", width=bar_width, color=current_palette[0])\n    plt.xticks(x, data[x_label], rotation=90)\n    plt.errorbar(x, y, yerr=(data[delta_lower_yerr_label], data[delta_upper_yerr_label]),\n                 fmt=\"none\", ecolor=error_color, capsize=2, capthick=True, elinewidth=1)\n\n    # Plot 2nd group of data\n    data = df2  # Pandas DataFrame\n    data[delta_lower_yerr_label] = data[y_label] - data[y_lower_label]\n    data[delta_upper_yerr_label] = data[y_upper_label] - data[y_label]\n    index = np.arange(df2.shape[0])\n\n    x = range(len(data[y_label]))\n    y = data[y_label]\n    ax.bar(index + bar_width, y, label = \"Empirical\", width=bar_width, color=current_palette[1])\n    plt.xticks(index + bar_width/2, data[x_label], rotation=90)\n    plt.errorbar(index + bar_width, y, yerr=(data[delta_lower_yerr_label], data[delta_upper_yerr_label]),\n                 fmt=\"none\", ecolor=error_color, capsize=2, capthick=True, elinewidth=1)\n\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    # create legend\n    from matplotlib.lines import Line2D\n    custom_lines = [Line2D([0], [0], color=current_palette[0], lw=5),\n                    Line2D([0], [0], color=current_palette[1], lw=5)]\n    ax.legend(custom_lines, [\"Physical\", \"Empirical\"])\n\ndef ridge_plot(df, by, column, figsize, colormap):\n    plt.rcParams['axes.labelsize'] = 14\n    plt.rcParams['xtick.labelsize'] = 14\n    plt.tight_layout()\n\n    # Make ridge plot\n    fig, axes = joypy.joyplot(data=df, by=by, column=column, figsize=figsize, colormap=colormap, linewidth=1)\n    # Add x-axis label\n    axes[-1].set_xlabel(column)\n\ndef ridge_plot_wo_overlap(df, by, column, figsize, colormap):\n        plt.rcParams['axes.labelsize'] = 14\n        plt.rcParams['xtick.labelsize'] = 14\n        plt.tight_layout()\n\n        # Make ridge plot\n        fig, axes = joypy.joyplot(data=df, by=by, column=column, figsize=figsize, colormap=colormap, linewidth=1, overlap=0)\n        # Add x-axis label\n        axes[-1].set_xlabel(column)\n\n# =============================================================================\n# UTILITY FUNCTIONS\n# =============================================================================\n\ndef read_collection_file(collection_file_path):\n    \"\"\"\n    Function to read SAMPL6 collection CSV file that was created by logPubmissionCollection.\n    :param collection_file_path\n    :return: Pandas DataFrame\n    \"\"\"\n\n    # Check if submission collection file already exists.\n    if os.path.isfile(collection_file_path):\n        print(\"Analysis will be done using the existing collection file: {}\".format(collection_file_path))\n\n        collection_df = pd.read_csv(collection_file_path, index_col=0)\n        print(\"\\n SubmissionCollection: \\n\")\n        print(collection_df)\n    else:\n        raise Exception(\"Collection file doesn't exist: {}\".format(collection_file_path))\n\n    return collection_df\n\n\ndef calc_MAE_for_molecules_across_all_predictions(collection_df, directory_path, file_base_name):\n    \"\"\"\n    Calculate mean absolute error for each molecule for all methods.\n    :param collection_df: Pandas DataFrame of submission collection.\n    :param directory_path: Directory for outputs\n    :param file_base_name: Filename for outputs\n    :return:\n    \"\"\"\n    # Create list of Molecule IDs\n    mol_IDs= list(set(collection_df[\"Molecule ID\"].values)) # List of unique IDs\n    mol_IDs.sort()\n    print(mol_IDs)\n\n    # List for keeping records of stats values for each molecule\n    molecular_statistics = []\n\n    # Slice the dataframe for each molecule to calculate MAE\n    for mol_ID in mol_IDs:\n        collection_df_mol_slice = collection_df.loc[collection_df[\"Molecule ID\"] == mol_ID]\n\n        # 2D array of matched calculated and experimental pKas\n        data = collection_df_mol_slice[[\"logP (calc)\", \"logP (exp)\"]].values\n\n        # Calculate MAE and RMSE and their 95% confidence intervals\n        bootstrap_statistics = compute_bootstrap_statistics(samples=data, stats_funcs=[mae, rmse], percentile=0.95,\n                                                                n_bootstrap_samples=10000)\n        MAE = bootstrap_statistics[0][0]\n        MAE_lower_CI = bootstrap_statistics[0][1][0]\n        MAE_upper_CI = bootstrap_statistics[0][1][1]\n        print(\"{} MAE: {} [{}, {}]\".format(mol_ID, MAE, MAE_lower_CI, MAE_upper_CI))\n\n        RMSE = bootstrap_statistics[1][0]\n        RMSE_lower_CI = bootstrap_statistics[1][1][0]\n        RMSE_upper_CI = bootstrap_statistics[1][1][1]\n        print(\"{} RMSE: {} [{}, {}]\\n\".format(mol_ID, RMSE, RMSE_lower_CI, RMSE_upper_CI))\n\n        # Record in CSV file\n        molecular_statistics.append({'Molecule ID': mol_ID, 'MAE': MAE, 'MAE_lower_CI': MAE_lower_CI,\n                                    'MAE_upper_CI': MAE_upper_CI, 'RMSE': RMSE, 'RMSE_lower_CI': RMSE_lower_CI,\n                                     'RMSE_upper_CI': RMSE_upper_CI})\n\n    # Convert dictionary to Dataframe to create tables/plots easily and save as CSV.\n    molecular_statistics_df = pd.DataFrame(molecular_statistics)\n    # Sort values by MAE values\n    molecular_statistics_df.sort_values(by='MAE', inplace=True)\n    \n    # Create directory if it doesn't exist\n    os.makedirs(directory_path, exist_ok=True)\n    \n    # Create CSV\n    file_base_path = os.path.join(directory_path, file_base_name)\n    with open(file_base_path + '.csv', 'w') as f:\n        molecular_statistics_df.to_csv(f)\n\n    # Plot MAE and RMSE of each molecule across predictions as a bar plot\n    barplot_with_CI_errorbars(df = molecular_statistics_df, x_label = 'Molecule ID',\n                              y_label = 'MAE', y_lower_label = 'MAE_lower_CI', y_upper_label = 'MAE_upper_CI',\n                              figsize=(8, 6))\n    plt.rcParams['xtick.labelsize'] = 12\n    plt.tight_layout()\n    plt.savefig(os.path.join(directory_path, \"MAE_vs_molecule_ID_plot.pdf\"))\n\n    barplot_with_CI_errorbars(df=molecular_statistics_df, x_label = 'Molecule ID',\n                              y_label = 'RMSE', y_lower_label = 'RMSE_lower_CI', y_upper_label = 'RMSE_upper_CI',\n                              figsize=(8, 6))\n    plt.rcParams['xtick.labelsize'] = 12\n    plt.tight_layout()\n    plt.savefig(os.path.join(directory_path, \"RMSE_vs_molecule_ID_plot.pdf\"))\n\n\ndef select_subsection_of_collection(collection_df, method_group):\n    \"\"\"\n    Returns a dataframe which is the subset of rows of collecion dataframe that match the requested method category\n    :param collection_df: Pandas DataFrame of submission collection.\n    :param method_group: String that specifies with method group is requested. \"Physical\",\"Empirical\",\"Mixed\" or \"Other\"\n    :return: Pandas DataFrame of subsection of submission collection.\n    \"\"\"\n\n    print(\"Looking for submissions of selected method group...\")\n    print(\"Method group: {}\".format(method_group))\n\n    # Filter collection dataframe based on method category\n    collection_df_of_selected_method_group = collection_df.loc[collection_df[\"category\"] == method_group]\n    collection_df_of_selected_method_group = collection_df_of_selected_method_group.reset_index(drop=True)\n    print(\"collection_df_of_selected_method_group: \\n {}\".format(collection_df_of_selected_method_group))\n\n    return collection_df_of_selected_method_group\n\n\ndef calc_MAE_for_molecules_across_selected_predictions(collection_df, selected_method_group, directory_path, file_base_name):\n    \"\"\"\n    Calculates mean absolute error for each molecule across prediction method category\n    :param collection_df: Pandas DataFrame of submission collection.\n\n    :param selected_method_group: \"Physical\", \"Empirical\", \"Mixed\", or \"Other\"\n    :param directory_path: Directory path for outputs\n    :param file_base_name: Output file name\n    :return:\n    \"\"\"\n\n    # Create subsection of collection dataframe for selected methods\n    collection_df_subset = select_subsection_of_collection(collection_df=collection_df, method_group=selected_method_group)\n\n    subset_directory_path = os.path.join(directory_path, selected_method_group)\n\n    # Calculate MAE using subsection of collection database\n    calc_MAE_for_molecules_across_all_predictions(collection_df=collection_df_subset, directory_path=subset_directory_path, file_base_name=file_base_name)\n\n\ndef create_comparison_plot_of_molecular_MAE_of_method_categories(directory_path, group1, group2, file_base_name):\n    \"\"\"\n    Create comparison plot of MAE for each molecule across method categories\n    \"\"\"\n    # Read molecular_error_statistics table\n    df_gr1 = pd.read_csv(os.path.join(directory_path, group1, \"molecular_error_statistics_for_{}_methods.csv\".format(group1)))\n    df_gr2 = pd.read_csv(os.path.join(directory_path, group2, \"molecular_error_statistics_for_{}_methods.csv\".format(group2)))\n\n    # Reorder dataframes based on the order of molecular MAE statistic of first group (Physical methods)\n    ordered_molecule_list = list(df_gr1[\"Molecule ID\"])\n    print(\"ordered_molecule_list: \\n\", ordered_molecule_list)\n\n    df_gr2_reordered = df_gr2.set_index(\"Molecule ID\")\n    df_gr2_reordered = df_gr2_reordered.reindex(index=df_gr1['Molecule ID']) #Reset row order based on index of df_gr1\n    df_gr2_reordered = df_gr2_reordered.reset_index()\n\n    # Plot\n    # Molecular labels will be taken from 1st dataframe, so the second dataframe should have the same molecule ID order.\n    barplot_with_CI_errorbars_and_2groups(df1=df_gr1, df2=df_gr2_reordered,\n                                          x_label=\"Molecule ID\", y_label=\"MAE\",\n                                          y_lower_label=\"MAE_lower_CI\", y_upper_label=\"MAE_upper_CI\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(directory_path, file_base_name + \".pdf\"))\n\n\ndef create_molecular_error_distribution_plots(collection_df, directory_path, file_base_name):\n    \"\"\"\n    Create ridge plots for error distributions\n    \"\"\"\n    # Ridge plot using all predictions\n    ridge_plot(df=collection_df, by=\"Molecule ID\", column=\"$\\Delta$logP error (calc - exp)\", figsize=(4, 6),\n                colormap=cm.plasma)\n    plt.savefig(os.path.join(directory_path, file_base_name + \"_all_methods.pdf\"))\n\n\n# =============================================================================\n# MAIN\n# =============================================================================\n\nif __name__ == '__main__':\n    # Create output directory structure\n    output_dir = 'pred_results/'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define paths\n    collection_file_path = 'benchmark/datasets/physical_properties/logP/analysis_of_extra_molecules/analysis_outputs_withrefs/logP_submission_collection.csv'\n    output_directory_path = os.path.join(output_dir, 'logP_analysis2_outputs')\n    os.makedirs(output_directory_path, exist_ok=True)\n    \n    # If the file doesn't exist, try to find it in the current directory structure\n    if not os.path.exists(collection_file_path):\n        possible_paths = [\n            './benchmark/datasets/analysis_outputs_withrefs/logP_submission_collection.csv',\n            './benchmark/datasets/logP_submission_collection.csv',\n            './benchmark/datasets/logP_submission_collection.csv'\n        ]\n        \n        for path in possible_paths:\n            if os.path.exists(path):\n                collection_file_path = path\n                break\n    \n    # Read collection file\n    collection_data = read_collection_file(collection_file_path=collection_file_path)\n    \n    # Create analysis directory\n    analysis_directory_name = 'MolecularStatisticsTables'\n    molecular_statistics_directory_path = os.path.join(output_directory_path, analysis_directory_name)\n    \n    # Remove directory if it already exists\n    if os.path.isdir(molecular_statistics_directory_path):\n        shutil.rmtree(molecular_statistics_directory_path)\n    \n    # Calculate MAE of each molecule across all predictions methods\n    calc_MAE_for_molecules_across_all_predictions(\n        collection_df=collection_data,\n        directory_path=molecular_statistics_directory_path,\n        file_base_name=\"molecular_error_statistics\"\n    )\n\n    # Calculate MAE for each molecule across each method category\n    list_of_method_categories = [\"Physical\", \"Empirical\"]\n    for category in list_of_method_categories:\n        calc_MAE_for_molecules_across_selected_predictions(\n            collection_df=collection_data,\n            selected_method_group=category,\n            directory_path=molecular_statistics_directory_path,\n            file_base_name=\"molecular_error_statistics_for_{}_methods\".format(category)\n        )\n\n    # Create comparison plot of MAE for each molecule across method categories\n    create_comparison_plot_of_molecular_MAE_of_method_categories(\n        directory_path=molecular_statistics_directory_path,\n        group1='Physical', \n        group2='Empirical',\n        file_base_name=\"molecular_MAE_comparison_between_method_categories\"\n    )\n\n    # Create molecular error distribution ridge plots for all methods\n    create_molecular_error_distribution_plots(\n        collection_df=collection_data,\n        directory_path=molecular_statistics_directory_path,\n        file_base_name=\"molecular_error_distribution_ridge_plot\"\n    )\n    \n    print(f\"Analysis complete. Results saved to {output_directory_path}\")\n```",
        "discipline": "chemistry",
        "file_url": "https://github.com/samplchallenges/SAMPL6/blob/master/physical_properties/logP/analysis_of_extra_molecules/logP_analysis2.py",
        "output_fname": "generated_gold_programs_hanane_partition/SAMPL6_logP_analysis2.py"
    },
    {
        "instruction": "You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks.\nGiven a user request, you are expected to write a complete program that accomplishes the requested task and save any outputs in the correct format.\nPlease wrap your program in a code block that specifies the script type, python. For example:\n```python\nprint(\"Hello World!\")\n```\n",
        "input": "Process protein interaction data from \"benchmark/datasets/Struct2Graph/pdb_files/input1/\" and \"benchmark/datasets/Struct2Graph/interactions_data.txt\" to generate training, testing, and development datasets for machine learning tasks. Filter protein pairs based on their presence in the input data and create interaction examples with shuffled indices and interaction labels. Save the complete dataset to \"pred_results/pred_all_examples.npy\" and \"pred_results/pred_create_examples.txt\". Additionally, split the dataset into training (80%), testing (10%), and development (10%) subsets, saving them as \"pred_results/pred_train_examples.npy\", \"pred_results/pred_one_to_one_test_examples.npy\", and \"pred_results/pred_one_to_one_dev_examples.npy\". Further, generate training subsets with positive-to-negative ratios of 1:2, 1:3, 1:5, and 1:10, saving them as \"pred_results/pred_train_examples_one_two.npy\", \"pred_results/pred_train_examples_one_three.npy\", \"pred_results/pred_train_examples_one_five.npy\", and \"pred_results/pred_train_examples_one_ten.npy\".",
        "output": "```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Jul  7 12:12:24 2020\n\n@author: mayank\nModified for standalone execution\n\"\"\"\n\nimport os\nimport numpy as np\nfrom random import shuffle\nimport os.path as osp\n\n# Create output directory if it doesn't exist\nos.makedirs(\"pred_results\", exist_ok=True)\n\n# Set paths correctly\nbase_dir = os.getcwd()\nstruct2graph_dir = \"benchmark/datasets/Struct2Graph\"\ninput_dir = osp.join(struct2graph_dir, \"pdb_files/input1/\")\n\n# Check if directory exists and create it if needed\nif not osp.exists(input_dir):\n    os.makedirs(input_dir, exist_ok=True)\n    print(f\"Created directory: {input_dir}\")\n\n# Get filenames in the input directory\ntry:\n    filenames = [f for f in os.listdir(input_dir) if osp.isfile(osp.join(input_dir, f))]\nexcept FileNotFoundError:\n    print(f\"Warning: Directory {input_dir} not found. Creating empty filenames list.\")\n    filenames = []\n\n# Filter for files that start with 'names'\nfiltered_filenames = [f for f in filenames if f.startswith('names')]\n        \np_list, m_list, s_list = [], [], []\n\n# Process each file if directory exists\nif filtered_filenames:\n    for f in filtered_filenames:\n        try:\n            a = np.load(osp.join(input_dir, f))\n            st_ind = int(f.strip().split('_')[1])\n            \n            for e, p in enumerate(a):\n                p_list.append(p)\n                m_list.append(st_ind)\n                s_list.append(e)\n        except Exception as e:\n            print(f\"Error processing file {f}: {e}\")\n\n# Read interactions data\ninteractions_file = osp.join(struct2graph_dir, 'interactions_data.txt')\ntry:\n    with open(interactions_file, 'r') as f:\n        protein_list = f.read().strip().split('\\n')\nexcept FileNotFoundError:\n    print(f\"Warning: Interactions file {interactions_file} not found. Using empty list.\")\n    protein_list = []\n\np1_list, p2_list, int_list = [], [], []\ni1_list, i2_list = [], []\nfor pair in protein_list:\n    p1, p2, int_val = pair.strip().split('\\t')\n    \n    if p1 in p_list and p2 in p_list:\n        p1_list.append(p1)\n        p2_list.append(p2)\n        int_list.append(int(int_val))\n        \n        i1 = p_list.index(p1)\n        i2 = p_list.index(p2)\n        \n        i1_list.append(m_list[i1]+s_list[i1]-1)\n        i2_list.append(m_list[i2]+s_list[i2]-1)\n        \n### Create Examples\nmerge_examples = []\n\nfor e, _ in enumerate(p1_list):\n    merge_examples.append([i1_list[e], i2_list[e], int_list[e]])\n\nshuffle(merge_examples)\nshuffle(merge_examples)\nshuffle(merge_examples)\n\nprint('Total number of examples : ' + str(len(merge_examples)))\n\n# Write results to output directory\noutput_dir = \"pred_results\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save all examples to text file\noutF = open(osp.join(output_dir, \"pred_create_examples.txt\"), \"w\")\nfor line in merge_examples:\n    outF.write(str(line[0]))\n    outF.write(\"\\t\")\n    outF.write(str(line[1]))\n    outF.write(\"\\t\")\n    outF.write(str(line[2]))\n    outF.write(\"\\n\")\noutF.close()\n\n# Save examples as numpy arrays\nnp.save(osp.join(output_dir, 'pred_all_examples'), merge_examples)\n\n### Create train, test and dev examples\nif merge_examples:\n    dataset_train_len = int(np.floor(0.8*len(merge_examples)))\n    dataset_rem_len = len(merge_examples)-dataset_train_len\n    dataset_test_len = int(np.floor(0.5*dataset_rem_len))\n\n    dataset_train = []\n    for i in range(dataset_train_len):\n        dataset_train.append(merge_examples[i])\n        \n    dataset_test = []\n    for i in range(dataset_train_len, dataset_train_len+dataset_test_len):\n        dataset_test.append(merge_examples[i])\n        \n    dataset_dev = []\n    for i in range(dataset_train_len+dataset_test_len, len(merge_examples)):\n        dataset_dev.append(merge_examples[i])\n        \n    np.save(osp.join(output_dir, 'pred_train_examples'), dataset_train)\n    np.save(osp.join(output_dir, 'pred_one_to_one_test_examples'), dataset_test)\n    np.save(osp.join(output_dir, 'pred_one_to_one_dev_examples'), dataset_dev)\n\n    ind_pos = []\n    ind_neg = []\n    for no, d in enumerate(dataset_train):\n        if d[-1] == 1:\n            ind_pos.append(no)\n        else:\n            ind_neg.append(no)\n        \n    pos_example = len(ind_pos)\n\n    if pos_example > 0:\n        # 1:2 Ratio\n        pos_inds = list(np.random.choice(np.array(ind_pos), max(1, int(pos_example/2)), replace=False))\n\n        dataset_train_12 = []\n        for no, d in enumerate(dataset_train):\n            if no in pos_inds or no in ind_neg:\n                dataset_train_12.append(d)\n                \n        shuffle(dataset_train_12)\n        np.save(osp.join(output_dir, 'pred_train_examples_one_two'), dataset_train_12)\n\n        # 1:3 ratio\n        pos_inds = list(np.random.choice(np.array(ind_pos), max(1, int(pos_example/3)), replace=False))\n\n        dataset_train_13 = []\n        for no, d in enumerate(dataset_train):\n            if no in pos_inds or no in ind_neg:\n                dataset_train_13.append(d)\n                \n        shuffle(dataset_train_13)\n        np.save(osp.join(output_dir, 'pred_train_examples_one_three'), dataset_train_13)\n\n        # 1:5 ratio\n        pos_inds = list(np.random.choice(np.array(ind_pos), max(1, int(pos_example/5)), replace=False))\n\n        dataset_train_15 = []\n        for no, d in enumerate(dataset_train):\n            if no in pos_inds or no in ind_neg:\n                dataset_train_15.append(d)\n                \n        shuffle(dataset_train_15)\n        np.save(osp.join(output_dir, 'pred_train_examples_one_five'), dataset_train_15)\n\n        # 1:10 ratio\n        pos_inds = list(np.random.choice(np.array(ind_pos), max(1, int(pos_example/10)), replace=False))\n\n        dataset_train_110 = []\n        for no, d in enumerate(dataset_train):\n            if no in pos_inds or no in ind_neg:\n                dataset_train_110.append(d)\n                \n        shuffle(dataset_train_110)\n        np.save(osp.join(output_dir, 'pred_train_examples_one_ten'), dataset_train_110)\n    else:\n        print(\"No positive examples found, skipping ratio-based datasets\")\nelse:\n    print(\"No examples found, skipping dataset creation\")\n\nprint(f\"All results saved to {output_dir}\")\n```",
        "discipline": "bioinformatics",
        "file_url": "https://github.com/baranwa2/Struct2Graph/blob/master/create_examples.py",
        "output_fname": "generated_gold_programs_hanane_partition_4/Struct2Graph_create_examples.py"
    },
    {
        "instruction": "You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks.\nGiven a user request, you are expected to write a complete program that accomplishes the requested task and save any outputs in the correct format.\nPlease wrap your program in a code block that specifies the script type, python. For example:\n```python\nprint(\"Hello World!\")\n```\n",
        "input": "Generate a comprehensive visualization of brain network data using the provided example datasets located in the 'netplotbrain' package. Specifically, utilize the node data from \"example_nodes.tsv\" and the edge data from \"example_edges.tsv\" to create a series of 3D plots that illustrate various aspects of brain connectivity. The output should be saved as a PNG file named \"showcase_image_pred.png\" in the \"pred_results/\" directory. Ensure that the visualizations highlight different views, node styles, and community templates, effectively showcasing the relationships and structures within the brain networks.",
        "output": "```python\n# In these examples we demonstrate how different templates\n\nimport numpy as np\nimport netplotbrain\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport templateflow.api as tf\nimport os\nimport pkg_resources\n\n# Create the output directory if it doesn't exist\nos.makedirs(\"pred_results/\", exist_ok=True)\n\n# Find the path to example data through the package resources\ntry:\n    # Get the example data directly from the netplotbrain package\n    nodes_path = pkg_resources.resource_filename('netplotbrain', 'example_data/example_nodes.tsv')\n    edges_path = pkg_resources.resource_filename('netplotbrain', 'example_data/example_edges.tsv')\nexcept:\n    # Fallback to the provided examples\n    nodes_path = 'benchmark/datasets/{dataset_folder_name}/example_nodes.tsv'\n    edges_path = 'benchmark/datasets/{dataset_folder_name}/example_edges.tsv'\n\n# Load the data\nnodes = pd.read_csv(nodes_path, sep='\\t', index_col=0)\nedges = pd.read_csv(edges_path, sep='\\t', index_col=0)\n\n# Create figure\nfig = plt.figure(figsize=(16, 12))\n\n# Plot 1\nax = fig.add_subplot(341, projection='3d')\nnetplotbrain.plot(nodes=nodes,\n                  edges=edges,\n                  template='MNI152NLin2009cAsym',\n                  template_style=None,\n                  view=['S'],\n                  node_type='spheres',\n                  node_alpha=0.5,\n                  node_color='Salmon',\n                  subtitles='Netplotbrain: plot networks',\n                  edge_weights='weight',\n                  edge_alpha=0.5,\n                  subtitle_fontsize=9,\n                  fig=fig, ax=ax)\n\n# Plot 2\nax = fig.add_subplot(342, projection='3d')\nnetplotbrain.plot(nodes=nodes,\n                  edges=edges,\n                  template='MNI152NLin2009cAsym',\n                  template_style='surface',\n                  view=['S'],\n                  node_type='spheres',\n                  node_alpha=0.5,\n                  node_color='Salmon',\n                  subtitles='Onto brains',\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  edge_widthscale=0.5)\n\n# Plot 3\nax = fig.add_subplot(343, projection='3d')\nnetplotbrain.plot(nodes=nodes,\n                  edges=edges,\n                  template='MNI152NLin2009cAsym',\n                  template_style='surface',\n                  view=['L'],\n                  node_type='spheres',\n                  node_alpha=0.5,\n                  node_color='Salmon',\n                  subtitles='At any viewing angle',\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  edge_widthscale=0.5)\n\n# Plot 4\nax = fig.add_subplot(344, projection='3d')\nnetplotbrain.plot(nodes=nodes,\n                  edges=edges,\n                  template='MNI152NLin2009cAsym',\n                  template_style='surface',\n                  view=['A'],\n                  node_type='spheres',\n                  node_alpha=0.5,\n                  node_color='community',\n                  node_size='centrality_measure2',\n                  subtitles='Easily set color and size',\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  node_cmap='gnuplot',\n                  edge_alpha=0.5,\n                  edge_widthscale=0.5,\n                  node_scale=10,\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 5\nax = fig.add_subplot(348, projection='3d')\nnetplotbrain.plot(template='MNI152NLin2009cAsym',\n                  template_style='cloudy',\n                  view='L',\n                  nodes=nodes,\n                  hemisphere='R',\n                  node_size='centrality_measure1',\n                  edges=edges,\n                  template_voxelsize=2,\n                  template_alpha=0.025,\n                  template_color='darkkhaki',\n                  subtitles='Different brain themes',\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 6\nax = fig.add_subplot(345, projection='3d')\nnetplotbrain.plot(nodes=nodes,\n                  edges=edges,\n                  template='MNI152NLin2009cAsym',\n                  template_style='surface',\n                  view=['S'],\n                  hemisphere=['L'],\n                  node_type='spheres',\n                  node_alpha=0.5,\n                  node_color='community',\n                  node_size='centrality_measure2',\n                  subtitles='Show one hemisphere',\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  node_cmap='gnuplot',\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 7\nax = fig.add_subplot(3, 4, 6, projection='3d')\nedgedf = pd.DataFrame()\nedgedf['i'] = [60, 95]\nedgedf['j'] = [95, 51]\nedgedf['weight'] = [1, 0.75]\nnodes_col = np.zeros([100])\nnodes_col[95] = 1\nnodes_col[[51, 60]] = 2\nnodes_seed = pd.DataFrame(data={'seed_roi': nodes_col})\nnetplotbrain.plot(template='MNI152NLin2009cAsym',\n                  template_style='surface',\n                  view='S',\n                  nodes={'atlas': 'Schaefer2018',\n                         'desc': '100Parcels7Networks',\n                         'resolution': 1},\n                  node_type='parcels',\n                  edges=edgedf,\n                  nodes_df=nodes_seed,\n                  template_alpha=0.03,\n                  subtitles='Different node styles',\n                  template_color='gray',\n                  highlight_nodes=[60, 95, 51],\n                  highlight_level=1,\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  edge_widthscale=3,\n                  node_color='seed_roi',\n                  edge_color='darkred',\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 8\nax = fig.add_subplot(3, 4, 7, projection='3d')\nnetplotbrain.plot(template='MNI152NLin2009cAsym',\n                  template_style='surface',\n                  view='L',\n                  nodes=nodes,  # Use the loaded nodes dataframe instead of path\n                  hemisphere=['R'],\n                  node_scale=5,\n                  edges=edges,  # Use the loaded edges dataframe instead of path\n                  template_alpha=0.05,\n                  subtitles='Highlight results',\n                  template_color='gray',\n                  highlight_level=0.9,\n                  highlight_nodes=[6, 8, 25, 29],\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 9\nax = fig.add_subplot(349, projection='3d')\nnetplotbrain.plot(nodes={'atlas': 'Schaefer2018',\n                         'desc': '400Parcels7Networks',\n                         'resolution': 1},\n                  template='MNI152NLin2009cAsym',\n                  template_style=None,\n                  view=['S'],\n                  subtitles='Plot a parcellation',\n                  subtitle_fontsize=9,\n                  node_type='parcels',\n                  node_alpha=0.5,\n                  node_color='Set3',\n                  fig=fig, ax=ax)\n\n# Plot 10\nax = fig.add_subplot(3, 4, 10, projection='3d')\natlasinfo = tf.get(template='MNI152NLin2009cAsym',\n                   atlas='Schaefer2018',\n                   desc='400Parcels7Networks',\n                   extension='.tsv')\natlasinfo = pd.read_csv(atlasinfo, sep='\\t')\n# Parse the info in to get network names\nnetworks = list(map(lambda x: x.split('_')[2], atlasinfo.name.values))\natlasinfo['yeo7networks'] = networks\n\nnetplotbrain.plot(nodes_df=atlasinfo,\n                  nodes={'atlas': 'Schaefer2018',\n                         'desc': '400Parcels7Networks',\n                         'resolution': 1},\n                  template='MNI152NLin2009cAsym',\n                  template_style=None,\n                  view=['S'],\n                  node_type='parcels',\n                  node_alpha=1,\n                  node_cmap='Dark2',\n                  node_color='yeo7networks',\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  subtitles='Community templates',\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 11\nax = fig.add_subplot(3, 4, 11, projection='3d')\nnetplotbrain.plot(nodes_df=atlasinfo,\n                  nodes={'atlas': 'Schaefer2018',\n                         'desc': '400Parcels7Networks',\n                         'resolution': 1},\n                  template='MNI152NLin2009cAsym',\n                  template_style=None,\n                  view=['S'],\n                  node_type='parcels',\n                  node_cmap='Dark2',\n                  node_color='yeo7networks',\n                  node_alpha=0.8,\n                  subtitles='Highlight a community',\n                  highlight_nodes={'yeo7networks': 'Cont'},\n                  highlight_level=0.95,\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Plot 12\nax = fig.add_subplot(3, 4, 12, projection='3d')\nnodes_whs = nodes.copy()\nnodes_whs['x'] = nodes_whs['x'] / 8\nnodes_whs['y'] = nodes_whs['y'] / 8\nnodes_whs['z'] = nodes_whs['z'] / 8\nnetplotbrain.plot(template='WHS',\n                  template_style='surface',\n                  view='S',\n                  nodes=nodes_whs,\n                  node_size='centrality_measure1',\n                  subtitles='TemplateFlow integration',\n                  edges=edges,\n                  node_scale=80,\n                  template_voxelsize=0.2,\n                  fig=fig, ax=ax,\n                  subtitle_fontsize=9,\n                  edge_widthscale=0.5,\n                  node_colorlegend=False,\n                  node_sizelegend=False)\n\n# Save the figure to the specified output directory\noutput_path = \"pred_results/showcase_image_pred.png\"\nfig.savefig(output_path, dpi=150)\nprint(f\"Figure saved to {output_path}\")\n```",
        "discipline": "neuroscience",
        "file_url": "https://github.com/wiheto/netplotbrain/blob/main/examples/showcase_image.py",
        "output_fname": "generated_gold_programs_hanane_partition/netplotbrain_showcase_image.py"
    },
    {
        "instruction": "You are an expert Python programming assistant that helps scientist users to write high-quality code to solve their tasks.\nGiven a user request, you are expected to write a complete program that accomplishes the requested task and save any outputs in the correct format.\nPlease wrap your program in a code block that specifies the script type, python. For example:\n```python\nprint(\"Hello World!\")\n```\n",
        "input": "Compute the linear correlation between two monovariate signals, where the first signal is a sinewave of 1 Hz frequency and the second signal is the sum of this sinewave with Gaussian noise (zero mean, unit variance). Standardize the signals to have zero mean and unit variance, and calculate the correlation function as a function of lag (up to 999 samples). Identify the maximum correlation value, the lag at which it occurs, and the Pearson correlation coefficient. Save the correlation function to \"pred_results/pred_Correlation.csv\" (columns: \"tau\" and \"correlation_function\"), a summary of the results to \"pred_results/pred_Correlation_summary.txt\", and visualizations of the input signals and the correlation function to \"pred_results/pred_Correlation_inputs.png\" and \"pred_results/pred_Correlation_result.png\", respectively.",
        "output": "```python\n\"\"\"\nCorrelation example:\nIt computes the linear correlation between two monovariate signals x and y (in DataFrame format) as a function of their delay tau.\nIt computes autocorrelation when y coincides with x. \n\"\"\"\n\n\"\"\" Import common python packages \"\"\"\nimport sys\nimport os\nimport numpy as np      # Mathematical package\nimport pandas as pd     # Time serie package\nimport matplotlib.pyplot as plt # Plotting package\nimport matplotlib\nmatplotlib.use('Agg')  # Use non-interactive backend\n\n# Create pred_results directory if it doesn't exist\nif not os.path.exists('pred_results'):\n    os.makedirs('pred_results')\n\nprint(\"\\n\")\nprint(\"***********************************************************************************************************************\")\nprint(\"This scripts computes the correlation between two monovariate signals.\"\n       \"First input is a sinewave of 1 Hz frequency, the second one\\n is the sum of this sinewave\"\n       \"with a gaussian random process having zero mean and unitary\\n variance.\")\nprint(\"************************************************************************************************************************\")\n\n# Define our own Correlation class with Python 3 compatible syntax\nclass Correlation:\n    \"\"\"\n    It computes the linear correlation between two monovariate signals x and y (in DataFrame format) as a function of their delay tau.\n    It computes autocorrelation when y coincides with x.\n    \"\"\"\n\n    def __init__(self, tau_max=100, plot=False, standardization=True, corr_tau_max=True, corr_coeff=True, scale=True):\n        \"\"\"\n        Constructor of Correlation instance\n        \n        :param tau_max:\n            The maximum lag (in samples) at which correlation should be computed\n        :type tau_max: int\n        \n        :param plot:\n            if True the plot of correlation function is returned. Default: False\n        :type plot: bool\n        \n        :param standardization:\n            if True, signals are standardized (to mean 0 and variance 1)\n        :type standardization: bool\n        \n        :param corr_tau_max:\n            if True, the maximum of the correlation function and its lag are returned\n        :type corr_tau_max: bool\n        \n        :param corr_coeff:\n            if True, the correlation coefficient (Pearson's version) is computed\n        :type corr_coeff: bool\n        \n        :param scale:\n            if True, correlation function is scaled in [-1,1]\n        :type scale: bool\n        \"\"\"\n        \n        self.tau_max = tau_max\n        self.plot = plot\n        self.standardization = standardization\n        self.corr_tau_max = corr_tau_max\n        self.corr_coeff = corr_coeff\n        self.scale = scale\n\n    def compute(self, signals):\n        \"\"\"\n        Computes the correlation between two signals\n        \n        :param signals:\n            List of two signals in DataFrame format, where each signal is a column of their respective dataframe.\n            The index of the DataFrame is the time of the sample.\n        :type signals: list\n        \n        :returns: dict\n            corr_funct: correlation function\n            max_corr: maximum of correlation function\n            t_max: lag at which maximum correlation occurs (in samples)\n            corr_coeff: correlation coefficient (Pearson's version)\n        :rtype: dict\n        \n        :raises: TypeError if signals parameter is not a list of two pandas.DataFrame\n        :raises: ValueError if signals parameter is a list with more or less than 2 elements\n        :raises: ValueError if signal dataframe contains more than one column\n        \"\"\"\n        \n        # Check input\n        if not isinstance(signals, list):\n            raise TypeError(\"signals parameter must be a list\")\n        \n        if len(signals) != 2:\n            raise ValueError(\"signals parameter must be a list of length 2\")\n        \n        x = signals[0]\n        y = signals[1]\n        \n        if not isinstance(x, pd.DataFrame) or not isinstance(y, pd.DataFrame):\n            raise TypeError(\"signals must be pandas DataFrames\")\n        \n        if x.shape[1] != 1 or y.shape[1] != 1:\n            raise ValueError(\"signals must have exactly one column\")\n        \n        # Get signals as numpy arrays\n        x = x.values.flatten()\n        y = y.values.flatten()\n        \n        # Standardize signals if requested\n        if self.standardization:\n            x = (x - np.mean(x)) / np.std(x)\n            y = (y - np.mean(y)) / np.std(y)\n        \n        # Compute correlation function\n        n = len(x)\n        corr_funct = np.zeros(2 * self.tau_max + 1)\n        \n        for tau in range(-self.tau_max, self.tau_max + 1):\n            idx = tau + self.tau_max\n            if tau < 0:\n                corr_funct[idx] = np.sum(x[-tau:] * y[:tau])\n            elif tau == 0:\n                corr_funct[idx] = np.sum(x * y)\n            else:\n                corr_funct[idx] = np.sum(x[:-tau] * y[tau:])\n        \n        # Scale correlation function if requested\n        if self.scale:\n            corr_funct = corr_funct / (np.sqrt(np.sum(x**2) * np.sum(y**2)))\n        \n        # Find maximum correlation and its lag if requested\n        max_corr = None\n        t_max = None\n        if self.corr_tau_max:\n            max_idx = np.argmax(np.abs(corr_funct))\n            max_corr = corr_funct[max_idx]\n            t_max = max_idx - self.tau_max\n        \n        # Compute correlation coefficient if requested\n        corr_coeff = None\n        if self.corr_coeff:\n            corr_coeff = np.corrcoef(x, y)[0, 1]\n        \n        # Plot correlation function if requested\n        if self.plot:\n            plt.figure(figsize=(10, 6))\n            plt.plot(range(-self.tau_max, self.tau_max + 1), corr_funct)\n            plt.title('Correlation function')\n            plt.xlabel('Lag (samples)')\n            plt.ylabel('Correlation')\n            if max_corr is not None and t_max is not None:\n                plt.plot(t_max, max_corr, 'ro')\n                plt.text(t_max, max_corr, f'  Max: {max_corr:.3f} at lag {t_max}')\n        \n        # Return results\n        result = {'corr_funct': corr_funct}\n        if max_corr is not None:\n            result['max_corr'] = max_corr\n        if t_max is not None:\n            result['t_max'] = t_max\n        if corr_coeff is not None:\n            result['corr_coeff'] = corr_coeff\n        \n        return result\n\n\"\"\" Define signals in pd.dataFrame format \"\"\"\n\n#Define parameters\nN=1024 # number of samples\nf=1.0  # sinewave frequency (Hz)\nFs=200 # sampling frequency (Hz)\n\nn=np.arange(0,N)#number of samples\n\n# Create signals\nx = pd.DataFrame({'X':np.sin(2*3.14*f*n/Fs)}, np.arange(0,N))\ny = pd.DataFrame({'Y':np.sin(2*3.14*f*n/Fs)+10*np.random.randn(1,N)[0]},np.arange(0,N))\n\n\"\"\"Plot input signals\"\"\"\nplt.figure(figsize=(10, 6))\nf, axarr = plt.subplots(2, sharex=True)\naxarr[0].set_title('Input signals')\naxarr[0].set_xlabel('Samples')\naxarr[1].set_xlabel('Samples')\n\naxarr[0].plot(n, x, label=\"x\")\naxarr[1].plot(n, y, label=\"y\", color='r')\naxarr[0].legend(loc='best')\naxarr[1].legend(loc='best')\n\n# Save the input signals plot\nplt.savefig('pred_results/pred_Correlation_inputs.png')\n\n\"\"\" Define class attributes of the wanted method \"\"\"\n\ntau_max = 999                       # the maximum lag at which correlation should be computed (in samples)\nplot=True                           # plot of the correlation function\nstandardization = True              # standardization of the time series to mean 0 and variance 1\ncorr_tau_max = True                 # return of the maximum of correlation and its lag\ncorr_coeff = True                   # computation of the correlation coefficient (Pearson's version)\nscale=True                          # scale factor to have correlaton in [-1,1]\n\n\"\"\" Instantiate the class with its attributes \"\"\"\nprint(\"\\n\")\n\ntry:\n    c = Correlation(tau_max, plot, standardization, corr_tau_max, corr_coeff, scale)\nexcept TypeError as err:\n    print(\"TypeError in Correlation constructor : \\n\" + str(err))\n    sys.exit(-1)\nexcept ValueError as err:\n    print(\"ValueError in Correlation constructor : \\n\" + str(err))\n    sys.exit(-1)\nexcept Exception as e:\n    print(\"Exception in Correlation constructor : \\n\" + str(e))\n    sys.exit(-1)\n\nprint(\"An instance the class is now created with the following parameters:\\n\" +\n      \"tau max = \" + str(tau_max) + \"\\n\" +\n      \"plot = \" + str(plot) + \"\\n\" +\n      \"standardization= \" + str(standardization) + \"\\n\" +\n      \"corr_tau_max = \" + str(corr_tau_max) + \"\\n\" +\n      \"corr_coeff =\" + str(corr_coeff) +\"\\n\" +\n      \"scale =\" + str(scale))\n\n\"\"\" Compute the method and get the result \"\"\"\nprint(\"\\n\")\nprint(\"Computing...\")\n\ntry:\n    res = c.compute([x, y])\nexcept TypeError as err:\n    print(\"TypeError in Correlation computation : \\n\" + str(err))\n    sys.exit(-1)\nexcept ValueError as err:\n    print(\"ValueError in Correlation computation : \\n\" + str(err))\n    sys.exit(-1)\nexcept Exception as e:\n    print(\"Exception in Correlation computation : \\n\" + str(e))\n    sys.exit(-1)\n\n\"\"\" Display result \"\"\"\nprint(\"\\n\")\nprint(\"**************************************** \\n\")\nprint('Correlation complete result :')\nprint(\"****************************************\\n\")\nprint(\"Correlation function array:\")\nprint(res['corr_funct'])\nprint(\"Maximum value of the correlation %f and lag (in samples) %d:\" %(res['max_corr'],res['t_max']))\nprint(\"Pearson's correlation coefficient %f:\" %(res['corr_coeff']))\n\n# Save the correlation plot that was generated by the Correlation class\nif plot:\n    plt.savefig('pred_results/pred_Correlation_result.png')\n\n# Save results to a CSV file\nresults_df = pd.DataFrame({\n    'tau': np.arange(-tau_max, tau_max+1),\n    'correlation_function': res['corr_funct']\n})\nresults_df.to_csv('pred_results/pred_Correlation.csv', index=False)\n\n# Also save the summary metrics\nwith open('pred_results/pred_Correlation_summary.txt', 'w') as f:\n    f.write(\"Maximum correlation: {}\\n\".format(res['max_corr']))\n    f.write(\"Lag at maximum correlation (samples): {}\\n\".format(res['t_max']))\n    f.write(\"Pearson's correlation coefficient: {}\\n\".format(res['corr_coeff']))\n\nprint(\"\\nResults saved to pred_results/pred_Correlation.csv and pred_results/pred_Correlation_summary.txt\")\nprint(\"Plots saved to pred_results/pred_Correlation_inputs.png and pred_results/pred_Correlation_result.png\")\n```",
        "discipline": "psychology",
        "file_url": "https://github.com/syncpy/SyncPy/blob/master/examples/Correlation.py",
        "output_fname": "generated_gold_programs_yifei_partition_3/SyncPy_Correlation.py"
    }
]